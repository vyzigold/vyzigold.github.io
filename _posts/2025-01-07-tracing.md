---
title: Tracing
categories:
- OpenStack
comments: true
feature_image: "https://vyzigold.github.io/placeholder.jpg"
---
A really interesting telemetry signals are [traces](https://opentelemetry.io/docs/concepts/signals/traces/). Traces can show us what happens in the application, where exactly it happens, how long it takes, what are the arguments and so on. Imagine for example creating a server in openstack. You use the `openstack server create` command. This creates a HTTP request to Nova, which then communicates with other components, like Glance, Neutron, Libvirt, the database, ... Ideally traces should enable us to see each of the HTTP requests between the components including which arguments were used and how long it took, we can also see the exact database SQL queries and so on.

Unfortunatelly Red Hat openstack doesn't include a way to collect traces out of the box (as of today), but there are 2 ways which I investigated and could be used to add this functionality to RHOSO or to any other flavor of openstack. You can either use the [OTEL autoinstrumentation](https://opentelemetry.io/docs/zero-code/python/) to instrument the components code or you can use an openstack project called [OSProfiler](https://github.com/openstack/osprofiler) which can do some of the OTEL autoinstrumentation too and it includes some additional manual instrumentation throughout a lot of the openstack services.

During my time looking into these I read a lot of opinions about which one is the better option, how many traces should be collected, which one is awfuly slow and so on. In this blog I can tell you how to start using each of them and I have a small comparison at the end as well, but I don't know for sure which option is better or what exact settings to use. That will require a lot more experimentation.

## OTEL Autoinstrumentation
The autoinstrumentation uses monkey patching of some of the Python libraries used by the openstack services like requests or sqlite. To use the autoinstrumentation you need to install the required instrumentation libraries and then run the `opentelemetry-instrument` script to which you'll give the service you want to instrument as an argument. You can also use the opentelemetry operator when running the openstack on top of kubernetes (for example RHOSO, but that needs some additional modifications)

### Example of autoinstrumenting on devstack

```bash
# install required packages (we don't need to install all the exporters, only those we want to use)
pip install opentelemetry-distro opentelemetry-exporter-otlp opentelemetry-exporter-jaeger opentelemetry-exporter-prometheus
opentelemetry-bootstrap -a install

# Edit the command run by systemd when runnig aodh-evaluator
sudo vim /etc/systemd/system/devstack\@aodh-evaluator.service

# Do the following replacement (configure only the exporters you really plan to use):
- ExecStart = /usr/local/bin/aodh-evaluator --config-file /etc/aodh/aodh.conf
+ ExecStart = /opt/stack/.local/bin/opentelemetry-instrument --traces_exporter console,otlp --metrics_exporter console,otlp,prometheus --service_name aodh-evaluator --exporter_otlp_endpoint 0.0.0.0:4317 --exporter_prometheus_host 0.0.0.0 --exporter_prometheus_port 3030 /usr/local/bin/aodh-evaluator --config-file /etc/aodh/aodh.conf

# Restart aodh-evaluator
sudo systemctl daemon-reload
sudo systemctl restart devstack@aodh-evaluator.service
```

With that you'll be able to see traces in the journal. You can then deploy and configure any kind of storage or transport solution you want (like Jaeger or OTEL collector). You can instrument other openstack services very similarly.

#### Example traces

Querying Prometheus
```json
 {
     "name": "GET",
     "context": {
         "trace_id": "0xb4021bc7d5ad9dbc96aa21f1943f2613",
         "span_id": "0x5edd7d131e47b2c3",
         "trace_state": "[]"
     },
     "kind": "SpanKind.CLIENT",
     "parent_id": null,
     "start_time": "2024-06-20T09:10:48.682452Z",
     "end_time": "2024-06-20T09:10:48.684403Z",
     "status": {
         "status_code": "UNSET"
     },
     "attributes": {
         "http.method": "GET",
         "http.url": "http://localhost:9090/api/v1/query?query=%28rate%28ceilometer_cpu"
         "http.status_code": 200
     },
     "events": [],
     "links": [],
     "resource": {
         "attributes": {
             "telemetry.sdk.language": "python",
             "telemetry.sdk.name": "opentelemetry",
             "telemetry.sdk.version": "1.25.0",
             "service.name": "aodh-evaluator",
             "telemetry.auto.version": "0.46b0"
         },
         "schema_url": ""
     }
 } 
```

DB query
```json
 {
     "name": "UPDATE aodh",
     "context": {
         "trace_id": "0xc83a87660dd7a8feb40509d9e0b92813",
         "span_id": "0xb5dfbebebbfb51dd",
         "trace_state": "[]"
     },
     "kind": "SpanKind.CLIENT",
     "parent_id": null,
     "start_time": "2024-06-20T08:50:48.659666Z",
     "end_time": "2024-06-20T08:50:48.660470Z",
     "status": {
         "status_code": "UNSET"
     },
     "attributes": {
         "db.statement": "UPDATE alarm SET evaluate_timestamp=%(evaluate_timestamp)s"
         "db.system": "mysql",
         "net.peer.name": "127.0.0.1",
         "db.name": "aodh",
         "db.user": "root"
     },
     "events": [],
     "links": [],
     "resource": {
         "attributes": {
             "telemetry.sdk.language": "python",
             "telemetry.sdk.name": "opentelemetry",
             "telemetry.sdk.version": "1.25.0",
             "service.name": "aodh-evaluator",
             "telemetry.auto.version": "0.46b0"
         },
         "schema_url": ""
     }
 }
```

#### Example of using the opentelemetry operator
- First you need to deploy the [opentelemetry operator](https://github.com/open-telemetry/opentelemetry-operator) either by applying the opentelemetry-operator.yaml or by using the helm chart (read the README in the link to know more).
- Then you need to create an Instrumentation CR, where you configure where the collected traces should go. For example I used this one:

```yaml
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: my-instrumentation
spec:
  propagators:
    - baggage
  python:
    env:
      # Required if endpoint is set to 4317.
      # Python autoinstrumentation uses http/proto by default
      # so data must be sent to 4318 instead of 4317.
      - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
        value: http://tempo-simplest-distributor:4318/v1/traces
      - name: OTEL_EXPORTER_OTLP_TRACES_INSECURE
        value: "true"
      - name: OTEL_METRICS_EXPORTER
        value: "none"
      - name: OTEL_LOGS_EXPORTER
        value: "none"
```
- Now add the following annotations to pod template annotations of a statefulset or deployment of each service you want to autoinstrument:
```yaml
        instrumentation.opentelemetry.io/enable-multi-instrumentation: "true"
        instrumentation.opentelemetry.io/inject-python: "true"
        instrumentation.opentelemetry.io/python-container-names: aodh-evaluator,aodh-notifier,aodh-listener
```
The configured pods should restart and you should start receiving traces in the place configured in the Instrumentation CR. It's teoretically also possible to use the apache-httpd instrumentation to get a different set of traces, but right now this would require the operators of the instrumented services to be disabled, which would probably eventually lead to a non-working stack, so it isn't really worth it right now. You also at the moment can't use multiple instrumentations at once with the opentelemetry-operator (as of the day I'm writing this) and python instrumentation seeems to provide more data.

## OSProfiler

The OSProfiler is an openstack project, which can use OTEL autoinstrumentation as well as custom manually added code througout a lot of the openstack services to generate traces and send them for remote storage. In comparison to plain OTEL autoinstrumentation, OSProfiler traces only requests with the correct HTTP X-Trace-HMAC=<hmac_key> header. On cli this means you'll need to add `--os-profile <hmac_key>` to the commands you want to trace. This can be used to limit the performance hit when enabling tracing.

Unfortunatelly I encountered some issues with some of the services (these might have been fixed since I wrote this blog post).
- When running all instrumentations, after, Nova stopped being able to create instances, they always stayed in the "BUILD" state. I think this might have been caused by the added OTEL autoinstrumentation (so by using the `trace_sqlalchemy` and `trace_requests`), but this would need further testing. I ended up disabling OSProfiler in Nova in my tests because of this.
- No matter what I did. I wasn't able to get any traces from Neutron. The Neutron code seems to be instrumented and it seems to allow using OSProfiler, but I wasn't able to get that code to do anything.
- When exporting traces to Jaeger through the thrift protocol (note that the library used by OSProfiler to do this is getting deprecated soon), I wasn't able to get any Nova traces in Jaeger. Nova traces would only appear in the journal, but they weren't being sent to the Jaeger. While Keystone traces were sent without a problem.

### Example of configuring OSProfiler

Modify a service config file, which you want to get traces from

```
[profiler_otlp]
service_name_prefix = my_keystone

[profiler]
enabled = True
trace_sqlalchemy = True
trace_requests = True
connection_string = otlp://127.0.0.1:4318
```

You can use multiple ways to export the traces. The above uses OTLP. You can use thrift to send traces to Jaeger with the following:
```
[profiler_jaeger]
service_name_prefix = my_keystone

[profiler]
enabled = True
trace_sqlalchemy = True
trace_requests = True
connection_string = jaeger://localhost:6831
```

## Final comparison and thoughts
I've shown you how to configure 2 different ways of getting traces from openstack services. The data you get are pretty similar, although OSProfiler uses some manual instrumentation and so it should be able to get all the interesting traces, while autoinstrumentation could miss something. OSProfiler is also easier to extend to collect additional data.

I tried to do some simple performance comparison of the 2 options. You can find the script I used as well as some detailed description of how I mesured the performance and the result in the following repository: [openstack-tracing-perftest](https://github.com/vyzigold/openstack-tracing-perftest/tree/main). In short the performance between OSProfiler and OTEL autoinstrumentation was pretty similar and the API calls in devstack were about 2% slower when compared to not using tracing in that particullar scenario I tested. Important note about the testing though. OSProfiler and OTEL autoinstrumentation have a few differences in the data they collect as explained above. I also had some difficulties with Nova and Neutron when using OSProfiler, so while the measured performance is similar, the amount of data each tool was collecting was unfortunatelly a little different, which certainly influenced the results. Another, different measurement needs to be done in order to be able to correctly compare the performance hit of each option.

